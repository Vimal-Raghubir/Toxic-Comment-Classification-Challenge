{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Toxic Comment Classification Challenge\n",
    "\n",
    "The goal of this challenge is to develop methods for detecting and classifying toxicity levels in online comments, aiming to outperform the state-of-the-art models available via the Perspective API. This notebook guides the process of training Machine Learning models to classify 6 toxicity types and analyzes results using metrics like accuracy, recall, precision, F1-score, and the challenge's main metric: mean column-wise ROC AUC."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1. Load in required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "import re\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, f1_score, recall_score, precision_score, roc_auc_score, confusion_matrix, classification_report\n",
    "from sklearn.svm import LinearSVC\n",
    "import spacy\n",
    "import numpy as np\n",
    "import pickle\n",
    "import os\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "from wordcloud import WordCloud\n",
    "from sklearn.exceptions import InconsistentVersionWarning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Suppress InconsistentVersionWarning present in some of the output\n",
    "warnings.filterwarnings(\"ignore\", category=InconsistentVersionWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2. Load the data\n",
    "**The train.csv file should be placed in a folder named train and the test data should be placed in a folder named test. If they aren't then the script won't work since the data will not be loaded in.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Used to load in the training and test data. \n",
    "train_df = pd.read_csv(\"train/train.csv\")\n",
    "test_df = pd.read_csv(\"test/test.csv\")\n",
    "test_labels = pd.read_csv(\"test/test_labels.csv\")\n",
    "print(len(train_df))\n",
    "print(len(test_df))\n",
    "print(len(test_labels))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build evaluation test set to match training data's structure\n",
    "The `train.csv` file contains 8 columns: an ID (unique row identifier), comment text, and 6 toxic class labels. In contrast, `test.csv` has only 2 columns (ID and comment text), while `test_labels.csv` includes the ID and 6 class labels.\n",
    "\n",
    "This structure works for generating a submission file matching the format of `sample_submission.csv`. However, to evaluate models, we need class labels to compare predictions. Some rows in `test_labels.csv` have -1 for all labels, indicating they are not used for scoring. The code below merges `test.csv` with `test_labels.csv`, excluding rows where all class labels are -1, leaving only rows used for evaluation to assess model performance locally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The data in test.csv \n",
    "\n",
    "# Prints the columns for all dataframes\n",
    "print(test_df.columns)\n",
    "print(test_labels.columns)\n",
    "print(train_df.columns)\n",
    "\n",
    "# Merges the test dataset into one since they both have the row ID columns\n",
    "merged_df = pd.merge(test_df, test_labels, on='id')\n",
    "\n",
    "# The class label columns\n",
    "label_columns = ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']\n",
    "\n",
    "# Filter out rows where the sum of the specified columns equals -6 (This indicates rows that are not used for scoring evaluation and therefore have placeholder labels)\n",
    "final_test_df = merged_df[merged_df[label_columns].sum(axis=1) != -6]\n",
    "\n",
    "# Display the final test dataframe\n",
    "final_test_df.head()\n",
    "\n",
    "print(len(final_test_df))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3. Perform EDA (Exploratory Data Analysis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.columns # lists all columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.describe() # There are no missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(train_df[train_df['comment_text'].isnull()]) # This checks the number of missing comments in the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4. Analyze class distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download punctuation tokens so they can be filtered out easily\n",
    "nltk.download('punkt')\n",
    "nltk.download('punkt_tab')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code below aims to create a column named `no_toxic_label` which counts the number of rows without a class label assigned (all class labels have a value of 0). This will help to gauge how many non toxic comments are present in the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a copy of the dataframe to make modifications without affecting the actual training data\n",
    "data_analysis = train_df.copy()\n",
    "\n",
    "# This sums the values for each row from the 2nd positional column (the first labelled column named toxic) until the final column\n",
    "row_sums = data_analysis.iloc[:, 2:].sum(axis=1)\n",
    "\n",
    "# Add a new column named no_toxic_label used to identify all unlabelled comments\n",
    "data_analysis['no_toxic_label'] = (row_sums == 0).astype(int)\n",
    "\n",
    "# Find rows where the sum of all class labels is equal to zero (indicating unlabelled comments)\n",
    "rows_with_zero_sum = data_analysis[row_sums == 0]\n",
    "\n",
    "print(f\"Count of rows that do not have any labels assigned to them indicating they are not toxic: {len(rows_with_zero_sum)}.\")  # Count of unlabelled comments\n",
    "\n",
    "print(f\"Percentage of all comments that have no toxic labels: {round(len(rows_with_zero_sum)/ len(train_df) * 100,2)}%.\") # We can see 89.83% of all comments have no labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Columns representing class labels\n",
    "classes = ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate', 'no_toxic_label']\n",
    "\n",
    "# Step 1: Precompute sentences and tokens for all comments\n",
    "data_analysis[\"sentence_count\"] = data_analysis[\"comment_text\"].apply(lambda text: len(sent_tokenize(text)))\n",
    "data_analysis[\"token_count\"] = data_analysis[\"comment_text\"].apply(lambda text: len(word_tokenize(text)))\n",
    "\n",
    "# Step 2: Aggregate counts for each class\n",
    "class_sentence_counts = {}\n",
    "class_token_counts = {}\n",
    "\n",
    "# Loop through each class label and filters all rows for rows that have a value of 1 in this current class label. Then the total sentence count and token count is computed for this class\n",
    "# and stored in the respective dictionaries\n",
    "for class_label in classes:\n",
    "    class_rows = data_analysis[data_analysis[class_label] == 1]\n",
    "    class_sentence_counts[class_label] = class_rows[\"sentence_count\"].sum()\n",
    "    class_token_counts[class_label] = class_rows[\"token_count\"].sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code below aims to print out for each class the the total tokens, total sentences, percentage of all tokens belonging to this class, percentage of all sentences belonging to this class, and its class distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This will simply get the total tokens and sentences in the entire dataset\n",
    "total_tokens = data_analysis[\"token_count\"].sum()\n",
    "total_sentences = data_analysis[\"sentence_count\"].sum()\n",
    "\n",
    "# Output results in a neat formatted way\n",
    "print(f\"{'Class':<15}{'Total Sentences':>20}{'Total Tokens':>20}{'Pct Sentences':>20}{'Pct Tokens':>20}\")\n",
    "print(\"-\" * 100)\n",
    "\n",
    "# For each class label, print out the class label, the total sentence count, total token count, percentage of all sentences belonging to this class and percentage of all tokens belonging to this class\n",
    "for class_label in classes:\n",
    "    total_sentences_class = class_sentence_counts[class_label]\n",
    "    total_tokens_class = class_token_counts[class_label]\n",
    "    percentage_sentences = round((total_sentences_class / total_sentences) * 100, 1)\n",
    "    percentage_tokens = round((total_tokens_class / total_tokens) * 100, 1)\n",
    "    \n",
    "    print(f\"{class_label:<15}{total_sentences_class:>20,}{total_tokens_class:>20,}{percentage_sentences:>20.1f}%{percentage_tokens:>20.1f}%\")\n",
    "\n",
    "print(\"-\" * 100)\n",
    "\n",
    "# Display class distribution summary\n",
    "print(\"\\nClass Distribution Summary:\")\n",
    "class_distribution = data_analysis[classes].sum()\n",
    "print(class_distribution)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code below aims to print out all of these distribution values in multiple bar charts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the labels and counts from the token dictionary\n",
    "labels = list(class_token_counts.keys())\n",
    "token_counts = list(class_token_counts.values())\n",
    "\n",
    "# Extract the counts from the sentence dictionary\n",
    "sentence_counts = list(class_sentence_counts.values())\n",
    "\n",
    "# Create a figure with 3 subplots (1 row, 3 columns)\n",
    "fig, axs = plt.subplots(1, 3, figsize=(18, 6))\n",
    "\n",
    "# Plot class distribution (binary values for each class)\n",
    "axs[0].bar(class_distribution.index, class_distribution.values, color='lightskyblue')\n",
    "axs[0].set_title('Class Distribution')\n",
    "axs[0].set_xlabel('Toxicity Classes')\n",
    "axs[0].set_ylabel('Number of Instances')\n",
    "axs[0].tick_params(axis='x', rotation=45)\n",
    "\n",
    "# Plot the sentence counts\n",
    "axs[1].bar(labels, sentence_counts, color='lightcoral')\n",
    "axs[1].set_title('Number of Sentences per Class')\n",
    "axs[1].set_xlabel('Toxicity Classes')\n",
    "axs[1].set_ylabel('Number of Sentences')\n",
    "axs[1].tick_params(axis='x', rotation=45)\n",
    "\n",
    "# Plot the token counts\n",
    "axs[2].bar(labels, token_counts, color='lightgreen')\n",
    "axs[2].set_title('Number of Tokens per Class')\n",
    "axs[2].set_xlabel('Toxicity Classes')\n",
    "axs[2].set_ylabel('Number of Tokens')\n",
    "axs[2].tick_params(axis='x', rotation=45)\n",
    "\n",
    "# Adjust layout to prevent overlap\n",
    "plt.tight_layout()\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final thoughts on class distribution\n",
    "\n",
    "Based on the analysis shown above, 90% of all comments have no toxic labels assigned to them indicating that only 10% of training examples will help the model to predict toxic comments. This is an unbalanced dataset that will lead inherently to more non toxic label predictions given the small amount of toxic comment examples. Besides this the toxic class has the most examples, sentences and tokens of the toxic class labels and it is considerably more than the lowest occurring class which would be threat, severe_toxic, and identity_hate. Threat, severe_toxic, and identity_hate collectively make up 3% of all training examples making it challenging for the model to predict these classes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Most Common Words per Class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code below prints out the top 10 most frequently occurring words per class to understand the most common words for each of them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize CountVectorizer with parameters to ignore common stop words\n",
    "vectorizer = CountVectorizer(stop_words='english', max_features=10)\n",
    "\n",
    "# Prepare a list to store results\n",
    "results = []\n",
    "\n",
    "# Calculate and print top words per class\n",
    "for class_label in classes:\n",
    "    # Filter comments for the current class\n",
    "    class_comments = data_analysis[data_analysis[class_label] == 1]['comment_text']\n",
    "    \n",
    "    # Transform the text into word frequency counts\n",
    "    word_counts = vectorizer.fit_transform(class_comments)\n",
    "    \n",
    "    # Get the most common words\n",
    "    common_words = vectorizer.get_feature_names_out()\n",
    "    word_freq = word_counts.sum(axis=0).A1\n",
    "\n",
    "    # Sort the words by frequency (highest to lowest)\n",
    "    sorted_word_freq, sorted_common_words = zip(*sorted(zip(word_freq, common_words), reverse=True))\n",
    "    \n",
    "    # Store results in the list\n",
    "    for word, freq in zip(sorted_common_words, sorted_word_freq):\n",
    "        results.append({\"Class\": class_label, \"Word\": word, \"Frequency\": freq})\n",
    "\n",
    "# Adjust pandas display settings to prevent truncation\n",
    "pd.set_option('display.max_rows', None)  # Show all rows\n",
    "pd.set_option('display.max_columns', None)  # Show all columns\n",
    "pd.set_option('display.max_colwidth', None)  # Show full width of each column\n",
    "pd.set_option('display.expand_frame_repr', False)  # Prevent wrapping of the table\n",
    "\n",
    "# Convert results into a pandas DataFrame\n",
    "results_df = pd.DataFrame(results)\n",
    "\n",
    "# Print or save the table\n",
    "print(results_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code below aims to print out the same shown above in multiple bar charts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create subplots\n",
    "fig, axs = plt.subplots(1, 7, figsize=(24, 8))\n",
    "\n",
    "# Loop through each class and plot the most common words\n",
    "for idx, class_label in enumerate(classes):\n",
    "    # Filter comments for the current class\n",
    "    class_comments = data_analysis[data_analysis[class_label] == 1]['comment_text']\n",
    "    \n",
    "    # Transform the text into word frequency counts\n",
    "    word_counts = vectorizer.fit_transform(class_comments)\n",
    "    \n",
    "    # Get the most common words and their frequencies\n",
    "    common_words = vectorizer.get_feature_names_out()\n",
    "    word_freq = word_counts.sum(axis=0).A1\n",
    "    \n",
    "    # Sort the words by frequency (highest to lowest)\n",
    "    sorted_word_freq, sorted_common_words = zip(*sorted(zip(word_freq, common_words), reverse=True))\n",
    "    \n",
    "    # Plot the sorted words and their frequencies\n",
    "    axs[idx].bar(sorted_common_words, sorted_word_freq, color='skyblue')\n",
    "    axs[idx].set_title(f\"Most Common Words for {class_label.capitalize()}\")\n",
    "    axs[idx].set_xlabel('Words')\n",
    "    axs[idx].set_ylabel('Frequency')\n",
    "    axs[idx].tick_params(axis='x', rotation=45)\n",
    "\n",
    "# Adjust layout to prevent overlap\n",
    "plt.tight_layout()\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code below aims to print a word cloud of most common words per class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create subplots (one for each class)\n",
    "fig, axs = plt.subplots(1, 7, figsize=(24, 6))\n",
    "\n",
    "# Loop through each class and generate a word cloud\n",
    "for idx, class_label in enumerate(classes):\n",
    "    # Filter comments for the current class\n",
    "    class_comments = data_analysis[data_analysis[class_label] == 1]['comment_text']\n",
    "    \n",
    "    # Combine all comments into a single string\n",
    "    combined_comments = \" \".join(class_comments)\n",
    "    \n",
    "    # Generate the word cloud\n",
    "    wordcloud = WordCloud(width=800, height=400, background_color='white').generate(combined_comments)\n",
    "    \n",
    "    # Plot the word cloud\n",
    "    axs[idx].imshow(wordcloud, interpolation='bilinear')\n",
    "    axs[idx].set_title(f\"Word Cloud for {class_label.capitalize()}\")\n",
    "    axs[idx].axis('off')  # Hide axes for better visualization\n",
    "\n",
    "# Adjust layout to prevent overlap\n",
    "plt.tight_layout()\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5. Preprocess the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    \"\"\"\n",
    "    Function aimed to perform text preprocessing on the comment text to prepare it for the feature extraction methods. \n",
    "    \"\"\"\n",
    "    # Lower case all letters\n",
    "    text = text.lower()\n",
    "    # Remove any unwanted characters and replace with a space\n",
    "    text = re.sub(r'[^a-zA-Z0-9\\s.,!?\\'\"-]', ' ', text)\n",
    "    # Replace multiple spaces with a single space\n",
    "    text = re.sub('\\s+', ' ', text)\n",
    "    # Removes any leading or trailing spaces\n",
    "    text = text.strip(' ')\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_df['comment_text'].head())  # Used to show the training comments before preprocessing\n",
    "train_df_preprocessed = train_df.copy() # Create a copy of the training data so we do not use the original dataframe\n",
    "train_df_preprocessed['comment_text'] = train_df['comment_text'].map(lambda text: preprocess_text(text))    # Pass each comment text into the function to be preprocessed\n",
    "print(train_df_preprocessed['comment_text'].head()) # Shows the results of the preprocessing on a few examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(final_test_df['comment_text'].head())  # Used to show the evaluation test comments before preprocessing\n",
    "test_df_preprocessed = final_test_df.copy() # Create a copy of the training data so we do not use the original dataframe\n",
    "test_df_preprocessed['comment_text'] = final_test_df['comment_text'].map(lambda text: preprocess_text(text))    # Pass each comment text into the function to be preprocessed\n",
    "print(test_df_preprocessed['comment_text'].head()) # Shows the results of the preprocessing on a few examples\n",
    "\n",
    "print(len(test_df_preprocessed))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(test_df['comment_text'].head())  # Used to show the submission test comments before preprocessing\n",
    "submission_test_df_preprocessed = test_df.copy() # Create a copy of the training data so we do not use the original dataframe\n",
    "submission_test_df_preprocessed['comment_text'] = submission_test_df_preprocessed['comment_text'].map(lambda text: preprocess_text(text))    # Pass each comment text into the function to be preprocessed\n",
    "print(submission_test_df_preprocessed['comment_text'].head()) # Shows the results of the preprocessing on a few examples\n",
    "\n",
    "print(len(submission_test_df_preprocessed))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6a. Use TFIDFVectorizer to vectorize the data\n",
    "To train ML algorithms on comment text, we need to convert it into a numeric representation that extracts key features. One method is TfidfVectorizer, which computes the term frequency-inverse document frequency (TF-IDF). This method assigns higher weights to terms that are frequent within a document but rare across the corpus, making them more informative. Common words like the, or, and for are de-emphasized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize a TfidfVectorizer to first fit our training data and then convert the evaluation and submission test datasets\n",
    "vectorizer = TfidfVectorizer(max_features=5000, stop_words='english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the comment text columns from all 3 preprocessed dataframes\n",
    "train_X = train_df_preprocessed['comment_text']\n",
    "test_X = test_df_preprocessed['comment_text']\n",
    "submission_test_X = submission_test_df_preprocessed['comment_text']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit and then transform the training comments\n",
    "train_X_vectorized = vectorizer.fit_transform(train_X)\n",
    "train_X_vectorized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform the evaluation test comments\n",
    "test_X_vectorized = vectorizer.transform(test_X)\n",
    "test_X_vectorized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform the submission test comments\n",
    "submission_test_X_vectorized = vectorizer.transform(submission_test_X)\n",
    "submission_test_X_vectorized"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6b. Use Spacy to convert the comments into word embeddings\n",
    "Another method is using word embeddings instead of traditional vectorized approaches. Pretrained vocabularies, like Word2Vec or GloVe, assign unique numeric arrays to words, capturing semantic relationships between them. By passing comment text through such a vocabulary, each word is converted into its numeric embedding, which can then be used to train ML algorithms. In this case we use SpaCy's vocabulary to create the word embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_word_embeddings(text, output_file):\n",
    "    \"\"\"\n",
    "        This function aims to load in a spacy model and convert the passed in text to its word embedding form. It will then save the data to file. If the embedding file already exists,\n",
    "        it will simply load it in instead of generating new embeddings to save on compute time.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # If the embedding file exists, load it\n",
    "        if os.path.exists(output_file):\n",
    "            print(f\"Loading embeddings from {output_file}.\")\n",
    "            embeddings = np.load(output_file)\n",
    "        else:\n",
    "            # Generate new embeddings\n",
    "            print(f\"The file {output_file} could not be found. Generating new embeddings...\")\n",
    "            nlp = spacy.load(\"en_core_web_md\")  # Load the spaCy model\n",
    "            embeddings = np.array([doc.vector for doc in nlp.pipe(text, batch_size=128)])  # Generate embeddings\n",
    "            \n",
    "            # Ensure the directory for the output file exists\n",
    "            os.makedirs(os.path.dirname(output_file), exist_ok=True)\n",
    "            \n",
    "            # Save the embeddings to the output file\n",
    "            np.save(output_file, embeddings)\n",
    "            print(f\"Embeddings saved to {output_file}.\")\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "        raise\n",
    "    return embeddings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This will get the embeddings for all training comment texts\n",
    "train_embeddings = generate_word_embeddings(train_df_preprocessed['comment_text'], \"embeddings/train_embeddings.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This will get the embeddings for all evaluation test comment texts\n",
    "test_embeddings = generate_word_embeddings(test_df_preprocessed['comment_text'], \"embeddings/test_embeddings.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This will get the embeddings for all submission test comment texts\n",
    "submission_train_embeddings = generate_word_embeddings(submission_test_df_preprocessed['comment_text'], \"embeddings/submission_train_embeddings.npy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7. Building Multilabel Classification Model Functions\n",
    "Since we aim to predict 6 class labels per comment, this is a multi-label problem. OneVsRestClassifier handles this by building 6 binary classifiers, each focusing on one label. For example, one classifier predicts whether a comment is toxic (1) or not toxic (0)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List all the class labels and access those columns from the training and evaluation test set\n",
    "label_columns = ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']\n",
    "train_y = train_df_preprocessed[label_columns]\n",
    "test_y = test_df_preprocessed[label_columns]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_logistic_regression_model(train_x, train_y, test_x, model_filename, submission_test_x=None, test_ids=None, submission_filename=None):\n",
    "    \"\"\"\n",
    "    Train a Logistic Regression model and optionally create a submission file.\n",
    "\n",
    "    Parameters:\n",
    "    - train_x: Training feature matrix (numpy array if it is word embeddings or sparse matrix if vectorized).\n",
    "    - train_y: Training target labels (pandas Dataframe).\n",
    "    - test_x: Test feature matrix for evaluation (numpy array if it is word embeddings or sparse matrix if vectorized).\n",
    "    - model_filename: File name to save the trained model.\n",
    "    - submission_test_x (optional): Feature matrix for submission predictions (numpy array if it is word embeddings or sparse matrix if vectorized).\n",
    "    - test_ids (optional): IDs corresponding to `submission_test_x`.\n",
    "    - submission_filename (optional): File name to save the submission file.\n",
    "    \"\"\"\n",
    "    logistic_regression_model = None\n",
    "\n",
    "    # Try to load the model if it exists, otherwise train a new one\n",
    "    try:\n",
    "        if os.path.exists(model_filename):\n",
    "            with open(model_filename, \"rb\") as f:\n",
    "                logistic_regression_model = pickle.load(f)\n",
    "        else:\n",
    "            raise FileNotFoundError(f\"Model file '{model_filename}' not found.\")\n",
    "    except FileNotFoundError as e:\n",
    "        print(e)\n",
    "        print(\"Training a new Logistic Regression model...\")\n",
    "        logistic_regression_model = OneVsRestClassifier(LogisticRegression(max_iter=5000, C=3, class_weight='balanced'))\n",
    "        logistic_regression_model.fit(train_x, train_y)\n",
    "\n",
    "        # Save the trained model to a file\n",
    "        os.makedirs(os.path.dirname(model_filename), exist_ok=True)  # Ensure the directory exists\n",
    "        with open(model_filename, \"wb\") as f:\n",
    "            pickle.dump(logistic_regression_model, f)\n",
    "        print(f\"Model saved to {model_filename}.\")\n",
    "\n",
    "    # If the submission filename, the test_ids are passed in, and the submission data are passed in then create the submission file\n",
    "    if (submission_filename and test_ids is not None and len(test_ids) > 0 and submission_test_x is not None and submission_test_x.getnnz() > 0):\n",
    "        # Generate predictions for the submission test dataset\n",
    "        submission_logistic_regression_predictions = logistic_regression_model.predict(submission_test_x)\n",
    "\n",
    "        # Create a DataFrame with the predictions and the corresponding IDs\n",
    "        submission_logistic_regression_predictions_df = pd.DataFrame(submission_logistic_regression_predictions, columns=train_y.columns)\n",
    "        submission_logistic_regression_predictions_df.insert(0, 'id', test_ids.reset_index(drop=True))\n",
    "        # Ensures the directory exists first before attempting to write the file\n",
    "        os.makedirs(os.path.dirname(submission_filename), exist_ok=True)  # Ensure the directory exists\n",
    "\n",
    "        # Save the predictions to a CSV file\n",
    "        submission_logistic_regression_predictions_df.to_csv(submission_filename, index=False, encoding='utf-8')\n",
    "        print(f\"Predictions saved to {submission_filename}.\")\n",
    "\n",
    "    # Predict on the test set\n",
    "    logistic_regression_predictions = logistic_regression_model.predict(test_x)\n",
    "    return logistic_regression_predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_svm_model(train_x, train_y, test_x, model_filename, submission_test_x=None, test_ids=None, submission_filename=None):\n",
    "    \"\"\"\n",
    "    Train a SVM model and optionally create a submission file.\n",
    "\n",
    "    Parameters:\n",
    "    - train_x: Training feature matrix (numpy array if it is word embeddings or sparse matrix if vectorized).\n",
    "    - train_y: Training target labels (pandas Dataframe).\n",
    "    - test_x: Test feature matrix for evaluation (numpy array if it is word embeddings or sparse matrix if vectorized).\n",
    "    - model_filename: File name to save the trained model.\n",
    "    - submission_test_x (optional): Feature matrix for submission predictions (numpy array if it is word embeddings or sparse matrix if vectorized).\n",
    "    - test_ids (optional): IDs corresponding to `submission_test_x`.\n",
    "    - submission_filename (optional): File name to save the submission file.\n",
    "    \"\"\"\n",
    "    svm_model = None\n",
    "\n",
    "    # Try to load the model if it exists, otherwise train a new one\n",
    "    try:\n",
    "        if os.path.exists(model_filename):\n",
    "            with open(model_filename, \"rb\") as f:\n",
    "                svm_model = pickle.load(f)\n",
    "        else:\n",
    "            raise FileNotFoundError(f\"Model file '{model_filename}' not found.\")\n",
    "    except FileNotFoundError as e:\n",
    "        print(e)\n",
    "        print(\"Training a new SVM model...\")\n",
    "        svm_model = OneVsRestClassifier(LinearSVC(random_state=42, max_iter=1000, class_weight=\"balanced\"))\n",
    "        svm_model.fit(train_x, train_y)\n",
    "\n",
    "        # Save the trained model to a file\n",
    "        os.makedirs(os.path.dirname(model_filename), exist_ok=True)  # Ensure the directory exists\n",
    "        with open(model_filename, \"wb\") as f:\n",
    "            pickle.dump(svm_model, f)\n",
    "        print(f\"Model saved to {model_filename}.\")\n",
    "\n",
    "    # If the submission filename, the test_ids are passed in, and the submission data are passed in then create the submission file\n",
    "    if (submission_filename and test_ids is not None and len(test_ids) > 0 and submission_test_x is not None and submission_test_x.getnnz() > 0):\n",
    "        # Generate predictions for the submission test dataset\n",
    "        submission_svm_predictions = svm_model.predict(submission_test_x)\n",
    "        # Create a DataFrame with the predictions and the corresponding IDs\n",
    "        submission_svm_predictions_df = pd.DataFrame(submission_svm_predictions, columns=train_y.columns)\n",
    "        submission_svm_predictions_df.insert(0, 'id', test_ids.reset_index(drop=True))\n",
    "\n",
    "        # Save the predictions to a CSV file\n",
    "        submission_svm_predictions_df.to_csv(submission_filename, index=False, encoding='utf-8')\n",
    "        print(f\"Predictions saved to {submission_filename}.\")\n",
    "\n",
    "    # Predict on test data\n",
    "    svm_predictions = svm_model.predict(test_x)\n",
    "    return svm_predictions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_random_forest_model(train_x, train_y, test_x, model_filename, submission_test_x=None, test_ids=None, submission_filename=None):\n",
    "    \"\"\"\n",
    "    Train a Random Forest model and optionally create a submission file.\n",
    "\n",
    "    Parameters:\n",
    "    - train_x: Training feature matrix (numpy array if it is word embeddings or sparse matrix if vectorized).\n",
    "    - train_y: Training target labels (pandas Dataframe).\n",
    "    - test_x: Test feature matrix for evaluation (numpy array if it is word embeddings or sparse matrix if vectorized).\n",
    "    - model_filename: File name to save the trained model.\n",
    "    - submission_test_x (optional): Feature matrix for submission predictions (numpy array if it is word embeddings or sparse matrix if vectorized).\n",
    "    - test_ids (optional): IDs corresponding to `submission_test_x`.\n",
    "    - submission_filename (optional): File name to save the submission file.\n",
    "    \"\"\"\n",
    "    random_forest_model = None\n",
    "\n",
    "    # Try to load the model if it exists, otherwise train a new one\n",
    "    try:\n",
    "        if os.path.exists(model_filename):\n",
    "            with open(model_filename, \"rb\") as f:\n",
    "                random_forest_model = pickle.load(f)\n",
    "        else:\n",
    "            raise FileNotFoundError(f\"Model file '{model_filename}' not found.\")\n",
    "    except FileNotFoundError as e:\n",
    "        print(e)\n",
    "        print(\"Training a new Random Forest model...\")\n",
    "        random_forest_model = OneVsRestClassifier(RandomForestClassifier())\n",
    "        random_forest_model.fit(train_x, train_y)\n",
    "\n",
    "        # Save the trained model to a file\n",
    "        os.makedirs(os.path.dirname(model_filename), exist_ok=True)  # Ensure the directory exists\n",
    "        with open(model_filename, \"wb\") as f:\n",
    "            pickle.dump(random_forest_model, f)\n",
    "        print(f\"Model saved to {model_filename}.\")\n",
    "\n",
    "    # If the submission filename, the test_ids are passed in, and the submission data are passed in then create the submission file\n",
    "    if (submission_filename and test_ids is not None and len(test_ids) > 0 and submission_test_x is not None and submission_test_x.getnnz() > 0):\n",
    "        # Generate predictions for the submission test dataset\n",
    "        submission_random_forest_predictions = random_forest_model.predict(submission_test_x)\n",
    "        # Create a DataFrame with the predictions and the corresponding IDs\n",
    "        submission_random_forest_predictions_df = pd.DataFrame(submission_random_forest_predictions, columns=train_y.columns)\n",
    "        submission_random_forest_predictions_df.insert(0, 'id', test_ids)\n",
    "\n",
    "        # Save the predictions to a CSV file\n",
    "        submission_random_forest_predictions_df.to_csv(submission_filename, index=False)\n",
    "        print(f\"Predictions saved to {submission_filename}.\")\n",
    "\n",
    "    random_forest_predictions = random_forest_model.predict(test_x)\n",
    "    return random_forest_predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 8. Train vectorized comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract test IDs to perform mapping operation\n",
    "test_ids = submission_test_df_preprocessed['id']\n",
    "\n",
    "# Logistic Regression model\n",
    "logistic_regression_predictions_vectorized = train_logistic_regression_model(train_X_vectorized, train_y, test_X_vectorized, \"models/Vectorized/Tuned/tuned_logistic_regression_vectorized.pkl\", submission_test_X_vectorized, test_ids, \"submission/vectorized_logistic_regression_submission.csv\")\n",
    "# SVM Model\n",
    "svm_predictions_vectorized = train_svm_model(train_X_vectorized, train_y, test_X_vectorized, \"models/Vectorized/Tuned/tuned_svm_vectorized.pkl\", submission_test_X_vectorized, test_ids, \"submission/vectorized_svm_submission.csv\")\n",
    "\n",
    "# Random Forest Model\n",
    "random_forest_predictions_vectorized = train_random_forest_model(train_X_vectorized, train_y, test_X_vectorized, \"models/Vectorized/Untuned/random_forest_classifier_vectorized.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 9. Evaluate Model Performance of vectorized approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model_performance(predictions, actual, model_name, feature_extraction, special_features=None):\n",
    "    \"\"\"\n",
    "        Function that takes in the predictions and actual labels and evaluate's the model's performance.\n",
    "\n",
    "        Parameters:\n",
    "        - predictions: predictions generated by the model\n",
    "        - actual: actual ground truth class labels\n",
    "        - model_name: the name of the model passed in as a string\n",
    "        - feature_extraction: the name of the method passed in as a string\n",
    "        - special_features (optional): list of all special hyperparameters for the model\n",
    "    \"\"\"\n",
    "    # Convert actual to numpy array if it is a DataFrame\n",
    "    actual = actual.values if isinstance(actual, pd.DataFrame) else actual\n",
    "\n",
    "    # Printing out metrics like accuracy, f1 score, precision score, and recall score\n",
    "    print(f\"Evaluating the {model_name} using {feature_extraction} to extract features, and evaluated the performance using: accuracy, F1 Score, Precision, Recall, mean column-wise ROC AUC.\")\n",
    "    if special_features:\n",
    "        print(f\"This model uses {special_features}.\")\n",
    "    print(f\"Also generated a Classification Report and Confusion Matrix.\")\n",
    "    print(\"Accuracy:\", accuracy_score(actual, predictions))\n",
    "    print(\"F1 Score (Micro):\", f1_score(actual, predictions, average='micro'))\n",
    "    print(\"Precision (Micro):\", precision_score(actual, predictions, zero_division=0, average='micro'))\n",
    "    print(\"Recall (Micro):\", recall_score(actual, predictions, average='micro'))\n",
    "\n",
    "    # Mean Column-Wise ROC AUC\n",
    "    try:\n",
    "        roc_auc_per_label = []\n",
    "        # This loops through each class label and generates the roc_auc score for each of them. Then append it to an array so we can simply compute the mean of all roc auc scores for\n",
    "        # the mean column-wise ROC AUC score.\n",
    "        for i, label in enumerate(label_columns):\n",
    "            roc_auc_label = roc_auc_score(actual[:, i], predictions[:, i])\n",
    "            roc_auc_per_label.append(roc_auc_label)\n",
    "        mean_roc_auc = np.mean(roc_auc_per_label)\n",
    "        print(\"Mean Column-Wise ROC AUC:\", mean_roc_auc)\n",
    "    except ValueError:\n",
    "        print(\"Mean Column-Wise ROC AUC could not be calculated. Ensure predictions are probabilities or scores.\")\n",
    "\n",
    "    print(\"\\nClassification Report:\\n\", classification_report(actual, predictions, zero_division=0, target_names=label_columns))\n",
    "    \n",
    "    # Set up subplots for the confusion matrices\n",
    "    fig, axes = plt.subplots(1, len(label_columns), figsize=(20, 4))\n",
    "    fig.suptitle(\"Confusion Matrices for Each Label\", fontsize=16)\n",
    "    \n",
    "    for i, label in enumerate(label_columns):\n",
    "        actual_label = actual[:, i]\n",
    "        predictions_label = predictions[:, i]\n",
    "        \n",
    "        cm = confusion_matrix(actual_label, predictions_label)\n",
    "        \n",
    "        # Plotting the confusion matrix for each label\n",
    "        sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", \n",
    "                    xticklabels=[\"Not \" + label, label], \n",
    "                    yticklabels=[\"Not \" + label, label],\n",
    "                    cbar=False, ax=axes[i])\n",
    "        axes[i].set_title(f\"'{label}'\")\n",
    "        axes[i].set_xlabel('Predicted')\n",
    "        axes[i].set_ylabel('Actual')\n",
    "    \n",
    "    plt.tight_layout(rect=[0, 0.03, 1, 0.95])  # Adjust layout to make room for the title\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Logistic Regression Model Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_model_performance(logistic_regression_predictions_vectorized, test_y, \"Logistic Regression model\", \"TfidfVectorizer\", \"a C value of 3, balanced class weight, and max iterations of 5000\")  # Evaluates performance of logistic regression that is tuned"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SVM Model Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_model_performance(svm_predictions_vectorized, test_y, \"SVM\", \"TfidfVectorizer\", \"C of 0.5, balanced class weight, and random state of 42\")  # Evaluates performance of SVM. Achieved best results using class_weighted of balanced"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random Forest Model Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_model_performance(random_forest_predictions_vectorized, test_y, \"Random Forest\", \"TfidfVectorizer\")  # Evaluates performance of Random Forest Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 10. Train embedding comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract test IDs to perform mapping operation\n",
    "test_ids = test_df_preprocessed['id']\n",
    "\n",
    "# Logistic Regression model\n",
    "logistic_regression_predictions_embeddings = train_logistic_regression_model(train_embeddings, train_y, test_embeddings, \"models/Embeddings/Tuned/tuned_logistic_regression_embeddings.pkl\")\n",
    "# SVM Model\n",
    "svm_predictions_embeddings = train_svm_model(train_embeddings, train_y, test_embeddings, \"models/Embeddings/Tuned/tuned_svm_classifier_embeddings.pkl\")\n",
    "\n",
    "# Random Forest Model. Only managed to train one random forest embedding model given the time it took to train\n",
    "random_forest_predictions_embeddings = train_random_forest_model(train_embeddings, train_y, test_embeddings, \"models/Embeddings/Tuned/tuned_random_forest_classifier_embeddings.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 11. Evaluate embeddings approach"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Logistic Regression Model Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_model_performance(logistic_regression_predictions_embeddings, test_y, \"Logistic Regression\", \"Word Embeddings\", \"a C value of 3, balanced class weight, and max iterations of 5000\")  # Evaluates performance of logistic regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SVM Model Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_model_performance(svm_predictions_embeddings, test_y, \"SVM\", \"Word Embeddings\", \"C of 0.5, balanced class weight, and random state of 42\")  # Evaluates performance of SVM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random Forest Model Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_model_performance(random_forest_predictions_embeddings, test_y, \"Random Forest\", \"Word Embeddings\")  # Evaluates performance of Random Forest"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
